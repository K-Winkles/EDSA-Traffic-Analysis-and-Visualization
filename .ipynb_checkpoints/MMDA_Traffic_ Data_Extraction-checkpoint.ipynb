{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDSA Traffic Analysis and Visualization: Extraction\n",
    "This script is in charge of extracting sufficient data. It is important to define the target_runs variable since this dictates the timeframe in which data will be extracted. Since the MMDA website live updates every 15 minutes, then the script must run accordingly.\n",
    "\n",
    "target_runs = number of days x 24 (hours) x 4 (extractions in 1 hour)\n",
    "\n",
    "The following libraries were imported:\n",
    "* For scheduling\n",
    "    1. schedule - used to track the runtime of the script and to prompt extraction at 15 minute intervals\n",
    "    2. time - used to take note of the time stamp.\n",
    "    3. datetime - used to format time.\n",
    "    4. os, os.path - used to count the amount of extracted files which will determine if the target number of runs has been reached.\n",
    "    5. exit (from sys) - used to stop script termination once the target number of runs has been achieved.\n",
    "\n",
    "Technically, the interval in which the script extracts data from the MMDA website is flexible. Hence, the same script can be used for other scraping tasks. \n",
    "\n",
    "* For visualization\n",
    "    1. numpy - in particular, numpy arrays were used for its flexibility.\n",
    "    2. matplotlib - this is a powerful and reliable visualization tool. \n",
    "\n",
    "* For Data Extraction\n",
    "    1. BeautifulSoup - used to process html data. \n",
    "    2. various selenium sublibraries - this was used to extract dynamic web content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define constants here\n",
    "global target_runs\n",
    "global interval\n",
    "\n",
    "target_runs = 2880 #30 days\n",
    "interval = 15 # in MINUTES\n",
    "\n",
    "#Libraries For Scheduling\n",
    "import schedule\n",
    "import time\n",
    "import datetime\n",
    "import os, os.path \n",
    "from sys import exit\n",
    "\n",
    "#Libraries for visualization\n",
    "%matplotlib notebook\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "\n",
    "#Libraries For Data Extraction\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import WebDriverException   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Extraction\n",
    "The flow is as follows:\n",
    "1. The webdriver from selenium will load the page in question. Of course, the url must first be specified. Note that to use webdriver, the appropriate driver must be installed. This has to be done in accordance to the browser being used. For example, Mozilla Firefox uses geckodriver, Google Chrome uses chromedriver, etc.\n",
    "2. Once the dynamic web content has been extracted, the data can now be processed using Beautiful Soup. The appropriate data will be extracted according to the webpage's html structure. \n",
    "3. Finally, after some pre-processing, the data is ready to be written to a .csv file and used for visualization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_extraction():\n",
    "    #specify the url\n",
    "    url = 'http://mmdatraffic.interaksyon.com/line-view-edsa.php'\n",
    "    \n",
    "    # Start the WebDriver and load the page\n",
    "    wd = webdriver.Firefox()\n",
    "    wd.get(url)\n",
    "\n",
    "    # Wait for the dynamically loaded elements to show up\n",
    "    WebDriverWait(wd, 10).until(\n",
    "        EC.visibility_of_element_located((By.CLASS_NAME, \"line-col\")))\n",
    "\n",
    "    # And grab the page HTML source\n",
    "    html_page = wd.page_source\n",
    "    wd.quit()\n",
    "\n",
    "    # Now you can use html_page as you like\n",
    "    soup = BeautifulSoup(html_page)\n",
    "    \n",
    "    #set time stamp.\n",
    "    ts = time.time()\n",
    "    timestamp = datetime.datetime.fromtimestamp(ts).strftime('%Y-%m-%d_%H_%M_%S')\n",
    "    stringdata = \"TIMESTAMP: \"+ str(timestamp)+\"\\n\"+\"\\n\"\n",
    "\n",
    "    #extract name of lines\n",
    "    list_of_names_html=soup.find_all('div',{'class':'line-name'})\n",
    "    list_of_names = []\n",
    "    \n",
    "\n",
    "    for children in list_of_names_html:\n",
    "        grandchildren = children.findChild(\"p\")\n",
    "        temp = grandchildren.get_text(separator=' ')\n",
    "        temp = temp.split(' ')\n",
    "        list_of_names.append(temp[0])\n",
    "    list_of_names.pop(0) \n",
    "    \n",
    "    #extract southbound/northbound volume\n",
    "    list_of_volume_html=soup.find_all('div',{'class':'line-status'})\n",
    "    list_of_southbound = []\n",
    "    list_of_northbound = []\n",
    "\n",
    "    i=1\n",
    "    for children in list_of_volume_html:\n",
    "        temp = children.text\n",
    "        temp = temp.split()\n",
    "\n",
    "        if(temp[1]==\"LIGHT\"):\n",
    "            temp[1]=0\n",
    "        elif(temp[1]==\"MODERATE\"):\n",
    "            temp[1]=1\n",
    "        elif(temp[1]==\"HEAVY\"):\n",
    "            temp[1]=2\n",
    "        elif(temp[1]==\"NO\"):\n",
    "            temp[1]=0 # Place holder first\n",
    "\n",
    "        if(i%2 == 0):\n",
    "            list_of_northbound.append(temp[1])\n",
    "        else:\n",
    "            list_of_southbound.append(temp[1])\n",
    "\n",
    "        i=i+1\n",
    "        \n",
    "    #save data into one variable\n",
    "    for i in range(len(list_of_names)):\n",
    "        stringdata = stringdata + '{:>12}  {:>12}  {:>12}'.format(list_of_names[i], str(list_of_southbound[i]), str(list_of_northbound[i])) + \"\\n\"\n",
    "    \n",
    "    #Save raw data into csv file\n",
    "    filename = \"rawdata_\"+ str(timestamp)+\".csv\"\n",
    "    file = open(filename,\"a\")\n",
    "    file.write(stringdata)   \n",
    "    file.close()\n",
    "\n",
    "    #return list containing data lists\n",
    "    list_of_lists = [list_of_names, list_of_southbound, list_of_northbound, timestamp]\n",
    "    return list_of_lists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization\n",
    "Note that the visualization and analysis of individual lines should be done on a different script. That is, the Data_Analysis script contained within the same repository. The purpose of this script is to focus on successfully extracting data. Hence, there is not much focus on visualization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Driver\n",
    "* The driver function contains the scheduling options for the script. As mentioned above, the interval in which data is extracted is flexible. Since the MMDA site updates every 15 minutes, the script extracts data accordingly. \n",
    "\n",
    "* Apart from scheduling options, it is also where essential functions are called so as to provide a structured workflow. Additionally, the metric determining whether the target number of runs has been hit is the amount of .png files in the directory. This will also work for .csv.\n",
    "\n",
    "#### Important notes:\n",
    "1. The consistency of data is dependent on the consistency of the network. Apart from that, it is also dependent on the consistency of the power being supplied to the machine. A suggestion to monitor the progress of the script is to use a VPN to remotely connect to the machine running the script. The easiest method to use for monitoring is to use Teamviewer.\n",
    "2. If there are inconsistencies in the data, this will also cause inconsistencies in the extraction since the script is run relative to the amount of files in the directory. This method of counting is used over an old-fashioned counter variable to prevent fatal index miscalculations (which might also arise when data is inconsistent). If inconsistencies are incurred, at the very least the worst that could happen is that there are too many rawdata files since the script is set to resume extraction no matter the cirucmstances (WebDriverException, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run number 1\n",
      "Run number 2\n",
      "Run number 3\n",
      "Run number 4\n",
      "Run number 5\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3304: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "def job():\n",
    "    try:      \n",
    "        #extract data at current time stamp\n",
    "        list_of_lists = data_extraction()\n",
    "\n",
    "        #track number of runs.\n",
    "        num_of_runs = (len([file for file in os.listdir('.') if file.endswith('.csv')]))\n",
    "\n",
    "        #if target of runs has been reached, visualize individual data.\n",
    "        if (num_of_runs == target_runs):\n",
    "            exit(0) \n",
    "    except WebDriverException:\n",
    "        # In case of power/network interruption, keep running \n",
    "        pass  \n",
    "\n",
    "schedule.every(interval).minutes.do(job)\n",
    "while 1:\n",
    "    schedule.run_pending()\n",
    "    time.sleep(1) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
